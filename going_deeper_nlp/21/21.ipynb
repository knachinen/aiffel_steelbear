{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de2f942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import collections\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sentencepiece as spm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# random seed 설정\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b6a909",
   "metadata": {},
   "source": [
    "# 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0fadb5",
   "metadata": {},
   "source": [
    "## SentencePiece 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c659298",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_file = os.getenv('HOME')+'/aiffel/bert_pretrain/data/kowiki.txt'\n",
    "\n",
    "vocab_size = 8000\n",
    "prefix = 'ko_' + str(vocab_size)\n",
    "\n",
    "data_dir = os.getenv('HOME')+'/aiffel/bert_pretrain/data'\n",
    "model_dir = os.getenv('HOME')+'/aiffel/bert_pretrain/models'\n",
    "\n",
    "if not os.path.exists(f\"{model_dir}/{prefix}.model\"):\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        f\"--input={corpus_file} --model_prefix={prefix}\" + # 말뭉치 & 저장 이름\n",
    "        f\" --vocab_size={vocab_size + 7}\" + # 사전 크기\n",
    "        \" --model_type=bpe\" +             # 토큰화 방식\n",
    "        \" --max_sentence_length=999999\" + # 문장 최대 길이\n",
    "        \" --pad_id=0 --pad_piece=[PAD]\" + # pad (0)\n",
    "        \" --unk_id=1 --unk_piece=[UNK]\" + # unknown (1)\n",
    "        \" --bos_id=2 --bos_piece=[BOS]\" + # begin of sequence (2)\n",
    "        \" --eos_id=3 --eos_piece=[EOS]\" + # end of sequence (3)\n",
    "        \" --user_defined_symbols=[SEP],[CLS],[MASK]\") # 사용자 정의 토큰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918a52cc",
   "metadata": {},
   "source": [
    "## SentencePiece 모델 불러오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3716cb69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(f\"{model_dir}/{prefix}.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f30f3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특수 토큰 7개를 제외한 모든 토큰 vocab_list에 저장\n",
    "vocab_list = []\n",
    "for id in range(7, len(vocab)):\n",
    "        if not vocab.is_unknown(id):\n",
    "            vocab_list.append(vocab.id_to_piece(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29064660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사전내 단어 개수: 8000\n"
     ]
    }
   ],
   "source": [
    "print('사전내 단어 개수:', len(vocab_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671b911a",
   "metadata": {},
   "source": [
    "# pre-train 데이터셋 준비하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642ee2c2",
   "metadata": {},
   "source": [
    "## 마스킹 함수 create_pretrain_mask 정의 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85c84c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n",
    "    \"\"\"\n",
    "    마스크 생성\n",
    "    :param tokens: 한 문장에 들어가는 tokens\n",
    "    :param mask_cnt: mask 개수 (전체 tokens의 15%)\n",
    "    :param vocab_list: vocab list (random token 용)\n",
    "    :return tokens: mask된 tokens\n",
    "    :return mask_idx: mask된 token의 index\n",
    "    :return mask_label: mask된 token의 원래 값\n",
    "    \"\"\"\n",
    "    # 단어 단위로 mask 하기 위해서 index 분할 (띄어쓰기)\n",
    "    cand_idx = []\n",
    "    for index, token in enumerate(tokens):\n",
    "        if token == '[CLS]' or token == '[SEP]':\n",
    "            # 특수 토큰인 [CLS]와 [SEP]은 무시\n",
    "            continue\n",
    "        if len(cand_idx) > 0 and not token.startswith(u'\\u2581'):\n",
    "            # 이전 토큰과 같은 단어로 취급\n",
    "            cand_idx[-1].append(index)\n",
    "        else:\n",
    "            # cand_idx에 단어 추가\n",
    "            cand_idx.append([index])\n",
    "\n",
    "    # random mask를 위해서 순서를 섞음 (shuffle)\n",
    "    random.shuffle(cand_idx)\n",
    "\n",
    "    # mask_lms 정렬 후 mask_idx, mask_label 추출 (sorted 사용)\n",
    "    mask_lms = []\n",
    "    for word_idx in cand_idx:\n",
    "        # 이미 mask_cnt만큼 마스킹 했다면 중지\n",
    "        if len(mask_lms) >= mask_cnt:\n",
    "            break\n",
    "        # word_idx의 모든 토큰을 추가할때 mask_cnt을 넘기면 다음 단어로 넘어감\n",
    "        if len(mask_lms) + len(word_idx) > mask_cnt:\n",
    "            continue\n",
    "        \n",
    "        # 단어 마스킹 방법 고르기\n",
    "        dice = random.random()\n",
    "\n",
    "        # 같은 단어에 속하는 토큰은 같은 마스킹 방법 적용\n",
    "        for index in word_idx:\n",
    "            # 80% 확률로 마스킹 토큰으로 대체\n",
    "            if dice < 0.8:\n",
    "                masked_token = \"[MSK]\"\n",
    "            # 10% 확률로 원래 토큰 유지\n",
    "            elif dice < 0.9:\n",
    "                masked_token = tokens[index]\n",
    "            # 10% 확률로 랜덤 토큰으로 대체\n",
    "            else:\n",
    "                masked_token = random.choice(vocab_list)\n",
    "\n",
    "            # 마스킹한 토큰의 위치와 원래 토큰 저장하기\n",
    "            mask_lms.append({'index': index, 'label': tokens[index]})\n",
    "            # 토큰 마스킹\n",
    "            tokens[index] = masked_token\n",
    "            \n",
    "    mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "    mask_idx = [p[\"index\"] for p in mask_lms]\n",
    "    mask_label = [p[\"label\"] for p in mask_lms]\n",
    "\n",
    "    return tokens, mask_idx, mask_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76cccd3",
   "metadata": {},
   "source": [
    "## create_pretrain_mask 함수 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3528a98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]'] \n",
      "\n",
      "tokens\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '[MSK]', '[MSK]', '[MSK]', '▁삼', '십', '▁전', '▁둘', '째', '번', '[MSK]', '[MSK]', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '프', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '[MSK]', '[MSK]', '[MSK]', '▁눈', '물이', '▁흘', '러', '[MSK]', '[MSK]', '[MSK]', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]'] \n",
      "\n",
      "mask_idx   : [23, 24, 25, 32, 33, 41, 57, 58, 59, 64, 65, 66, 79, 80]\n",
      "mask_label : ['▁첫', '▁번', '에', '▁오', '십', '▁십', '▁기', '쁨', '의', '▁컬', '컬', '한', '▁전', '부터']\n"
     ]
    }
   ],
   "source": [
    "# 입력 문장 생성\n",
    "string_a = \"추적추적 비가 내리는 날이었어 그날은 왠지 손님이 많아 첫 번에 삼십 전 둘째번 오십 전 오랜만에 받아보는 십 전짜리 백통화 서푼에\"\n",
    "string_b = \"손바닥 위엔 기쁨의 눈물이 흘러 컬컬한 목에 모주 한잔을 적셔 몇 달 포 전부터 콜록거리는 아내 생각에 그토록 먹고 싶다던\"\n",
    "tokens = [\"[CLS]\"] + vocab.encode_as_pieces(string_a) + [\"[SEP]\"] + vocab.encode_as_pieces(string_b) + [\"[SEP]\"]\n",
    "print(tokens, \"\\n\")\n",
    "\n",
    "mask_cnt = int(len(tokens) * 0.15)\n",
    "\n",
    "# 문장 마스킹\n",
    "tokens, mask_idx, mask_label = create_pretrain_mask(tokens, mask_cnt, vocab_list)\n",
    "\n",
    "# 마스킹된 결과\n",
    "print(\"tokens\")\n",
    "print(tokens, \"\\n\")\n",
    "\n",
    "# 마스킹한 토큰\n",
    "print(\"mask_idx   :\", mask_idx)\n",
    "print(\"mask_label :\", mask_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871cbe73",
   "metadata": {},
   "source": [
    "## NSP pair 생성 함수  create_pretrain_instances 정의\n",
    "- `trim_tokens`\n",
    "    - 문장의 길이를 일정 길이(max_seq)에 맞추는 함수\n",
    "- `create_pretrain_instances`\n",
    "    - 여러 문장이 담긴 doc을 받아 MLM과 NSP처리\n",
    "    - 반환 타입\n",
    "        - tokens\n",
    "            - MLM과 NSP 처리된 시퀀스\n",
    "        - segment\n",
    "            - segment embedding에 전달할 세그먼트 구분 정보\n",
    "            - 문장 A와 문장 B이 어디까지인지 표시\n",
    "        - is_next\n",
    "            - NSP 타겟 데이터\n",
    "            - 문장 연결이 매끄러운지의 여부\n",
    "                - 1: True(isNext)\n",
    "                - 2: False(notNext)\n",
    "        - mask_idx\n",
    "            - MLM 타겟 데이터\n",
    "            - 마스킹된 토큰의 인덱스 리스트\n",
    "        - mask_label\n",
    "            - MLM 타겟 데이터\n",
    "            - 마스킹된 토큰의 정답 토큰 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6ad1a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_tokens(tokens_a, tokens_b, max_seq):\n",
    "    \"\"\"\n",
    "    tokens_a, tokens_b의 길이를 줄임 최대 길이: max_seq\n",
    "    :param tokens_a: tokens A\n",
    "    :param tokens_b: tokens B\n",
    "    :param max_seq: 두 tokens 길이의 최대 값\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        # 문장 길이가 max_seq 안으로 들어오면 종료\n",
    "        if total_length <= max_seq:\n",
    "            break\n",
    "        \n",
    "        # 두 시퀀스 중 가장 긴 시퀀스의 토큰 제거\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            del tokens_a[0]\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a533715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list):\n",
    "    \"\"\"\n",
    "    doc별 pretrain 데이터 생성\n",
    "    :param vocab: SentencePieceProcess(), 토큰화 객체\n",
    "    :param doc: MLM과 NSP 데이터로 만들 document, 각 문장마다 토큰화된 sentence 리스트\n",
    "    :param n_seq: 최종 시퀀스의 최대 길이\n",
    "    :param mask_prob: 각 토큰의 마스킹 확률\n",
    "    :vocab_list: 특수 토큰을 제외한 사전 내 토큰 리스트\n",
    "    :return instances: MLM과 NSP 처리된 학습 데이터셋, 위의 markdown 참조\n",
    "    \"\"\"\n",
    "    # 특수 토큰 [CLS]과 [SEP]는 필수이므로 길이 제한에서 제외\n",
    "    max_seq = n_seq - 3\n",
    "\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for i in range(len(doc)):\n",
    "        current_chunk.append(doc[i])  # line 단위로 추가\n",
    "        current_length += len(doc[i])  # current_chunk의 token 수\n",
    "        \n",
    "        # 마지막 줄 이거나 길이가 max_seq 이상 인 경우\n",
    "        if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  \n",
    "            # tokens_a와 tokens_b로 분리하기 위해 경계 고르기\n",
    "            a_end = 1\n",
    "            if 1 < len(current_chunk):\n",
    "                a_end = random.randrange(1, len(current_chunk))\n",
    "                \n",
    "            # tokens_a 가져오기\n",
    "            tokens_a = []\n",
    "            for j in range(a_end):\n",
    "                tokens_a.extend(current_chunk[j])\n",
    "                \n",
    "            # tokens_b 가져오기\n",
    "            tokens_b = []\n",
    "            for j in range(a_end, len(current_chunk)):\n",
    "                tokens_b.extend(current_chunk[j])\n",
    "\n",
    "            # 50% 확률로 notNext로 변환\n",
    "            if random.random() < 0.5:  # 50% 확률로 swap\n",
    "                is_next = 0    # False\n",
    "                tokens_t = tokens_a\n",
    "                tokens_a = tokens_b\n",
    "                tokens_b = tokens_t\n",
    "            else:\n",
    "                is_next = 1   # True\n",
    "                \n",
    "            # max_seq 보다 큰 경우 길이 조절\n",
    "            trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "            assert 0 < len(tokens_a)\n",
    "            assert 0 < len(tokens_b)\n",
    "\n",
    "            # tokens & segment 생성\n",
    "            tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "            segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "            \n",
    "            # Masked Language Model 데이터 생성\n",
    "            tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * 0.15), vocab_list)\n",
    "\n",
    "            instance = {\n",
    "                \"tokens\": tokens,\n",
    "                \"segment\": segment,\n",
    "                \"is_next\": is_next,\n",
    "                \"mask_idx\": mask_idx,\n",
    "                \"mask_label\": mask_label\n",
    "            }\n",
    "            instances.append(instance)\n",
    "\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3887a23",
   "metadata": {},
   "source": [
    "## create_pretrain_instances 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b564f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트용 데이터\n",
    "string = \"\"\"추적추적 비가 내리는 날이었어\n",
    "그날은 왠지 손님이 많아\n",
    "첫 번에 삼십 전 둘째 번 오십 전\n",
    "오랜만에 받아보는 십 전짜리 백통화 서푼에\n",
    "손바닥 위엔 기쁨의 눈물이 흘러\n",
    "컬컬한 목에 모주 한잔을 적셔\n",
    "몇 달 포 전부터 콜록거리는 아내\n",
    "생각에 그토록 먹고 싶다던\n",
    "설렁탕 한 그릇을 이제는 살 수 있어\n",
    "집으로 돌아가는 길 난 문득 떠올라\n",
    "아내의 목소리가 거칠어만 가는 희박한 숨소리가\n",
    "오늘은 왠지 나가지 말라던 내 옆에 있어 달라던\n",
    "그리도 나가고 싶으면 일찍이라도 들어와 달라던\n",
    "아내의 간절한 목소리가 들려와\n",
    "나를 원망하듯 비는 점점 거세져\n",
    "싸늘히 식어가는 아내가 떠올라 걱정은 더해져\n",
    "난 몰라 오늘은 운수 좋은 날\n",
    "난 맨날 이렇게 살 수 있으면 얼마나 좋을까\"\"\"\n",
    "\n",
    "# 데이터 토큰화\n",
    "doc = [vocab.encode_as_pieces(line) for line in string.split(\"\\n\")]\n",
    "doc[:3]\n",
    "\n",
    "# 최대 길이\n",
    "n_test_seq = 64\n",
    "# 최소 길이\n",
    "min_seq = 8\n",
    "# [CLS], tokens_a, [SEB], tokens_b, [SEP]\n",
    "max_seq = n_test_seq - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "736fb7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '[MSK]', '[MSK]', '▁전', '▁둘', '째', '[MSK]', '▁오', '십', '[MSK]', '▁오', '랜', '만에', '▁받아', '보', '는', '[MSK]', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '[SEP]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '[MSK]', '[MSK]', '[MSK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [14, 15, 16, 19, 22, 29, 60, 61, 62], 'mask_label': ['▁삼', '십', '▁전', '▁번', '▁전', '▁십', '▁날', '이었', '어']}\n",
      "{'tokens': ['[CLS]', '[MSK]', '[MSK]', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라', '▁아내', '의', '[MSK]', '[MSK]', '[MSK]', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가', '[SEP]', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '[MSK]', '[MSK]', '▁싶', '다', '던', '▁설', '렁', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 2, 14, 15, 16, 32, 33, 56, 57], 'mask_label': ['▁집', '으로', '▁목', '소', '리가', '▁목', '에', '▁먹', '고']}\n",
      "{'tokens': ['[CLS]', '[MSK]', '▁있어', '▁달', '라', '던', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '라', '던', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '[SEP]', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '▁거', '세', '져', '▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '으로서', '▁종종', '▁늘', '[MSK]', '[MSK]', '[MSK]', '▁더', '해', '져', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 39, 40, 54, 55, 56, 57, 58, 59], 'mask_label': ['에', '▁비', '는', '▁떠', '올', '라', '▁', '걱', '정은']}\n",
      "{'tokens': ['[CLS]', '▁난', '▁몰', '라', '▁오늘', '은', '▁운', '수', '▁좋은', '[MSK]', '[SEP]', '▁난', '▁맨', '날', '▁이렇게', '[MSK]', '[MSK]', '▁있', '으면', '▁얼마', '나', '▁좋', '을', '까', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [9, 15, 16], 'mask_label': ['▁날', '▁살', '▁수']}\n"
     ]
    }
   ],
   "source": [
    "# NSP 데이터 생성\n",
    "instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "\n",
    "# NSP 데이터 생성 결과\n",
    "for instance in instances:\n",
    "    print(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841676aa",
   "metadata": {},
   "source": [
    "## pre-train 데이터셋 생성 함수 make_pretrain_data 정의 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8977f5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretrain_data(vocab, in_file, out_file, n_seq, mask_prob=0.15):\n",
    "    \"\"\" \n",
    "    pretrain 데이터 생성 \n",
    "    :param vocab: SentencePieceProcess(), 토큰화 객체\n",
    "    :param in_file: pretrain에 사용될 데이터 들어있는 파일 위치\n",
    "    :param out_file: 생성된 pretrain 데이터가 저장될 파일 위치\n",
    "    :param n_seq: 최종 시퀀스의 최대 길이\n",
    "    :param mask_prob: 각 토큰의 마스킹 확률\n",
    "    \"\"\"\n",
    "    def save_pretrain_instances(out_f, doc):\n",
    "        instances = create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list)\n",
    "        for instance in instances:\n",
    "            out_f.write(json.dumps(instance, ensure_ascii=False))\n",
    "            out_f.write(\"\\n\")\n",
    "\n",
    "    # 특수문자 7개를 제외한 vocab_list 생성\n",
    "    vocab_list = []\n",
    "    for id in range(7, len(vocab)):\n",
    "        if not vocab.is_unknown(id):        # 생성되는 단어 목록이 unknown인 경우는 제거합니다. \n",
    "            vocab_list.append(vocab.id_to_piece(id))\n",
    "\n",
    "    # line count 확인\n",
    "    line_cnt = 0\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            line_cnt += 1\n",
    "\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        with open(out_file, \"w\") as out_f:\n",
    "            doc = []\n",
    "            for line in tqdm(in_f, total=line_cnt):\n",
    "                line = line.strip()\n",
    "                if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)\n",
    "                    if 0 < len(doc):\n",
    "                        save_pretrain_instances(out_f, doc)\n",
    "                        doc = []\n",
    "                else:  # line이 빈줄이 아닐 경우 tokenize 해서 doc에 저장\n",
    "                    pieces = vocab.encode_as_pieces(line)    \n",
    "                    if 0 < len(pieces):\n",
    "                        doc.append(pieces)\n",
    "            if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "                save_pretrain_instances(out_f, doc)\n",
    "                doc = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed95615",
   "metadata": {},
   "source": [
    "## make_pretrain_data를 통해 pretrain 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "435265c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c11215291074cca98963e676687f99c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretrain_json_path = os.getenv('HOME') + '/aiffel/bert_pretrain/data/bert_pre_train.json'\n",
    "\n",
    "if not os.path.exists(pretrain_json_path):\n",
    "    make_pretrain_data(vocab, corpus_file, pretrain_json_path, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6310d10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 수: 918189\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 크기 확인\n",
    "total = 0\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        total += 1\n",
    "print(\"데이터 수:\", total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba193b05",
   "metadata": {},
   "source": [
    "# pretrain 데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b6bd326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pre_train_data(vocab, filename, n_seq, count=None):\n",
    "    \"\"\"\n",
    "    학습에 필요한 데이터를 로드\n",
    "    :param vocab: vocab\n",
    "    :param filename: 전처리된 json 파일\n",
    "    :param n_seq: 시퀀스 길이 (number of sequence)\n",
    "    :param count: 데이터 수 제한 (None이면 전체)\n",
    "    :return enc_tokens: encoder inputs\n",
    "    :return segments: segment inputs\n",
    "    :return labels_nsp: nsp labels\n",
    "    :return labels_mlm: mlm labels\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            total += 1\n",
    "            # 데이터 수 제한\n",
    "            if count is not None and count <= total:\n",
    "                break\n",
    "    \n",
    "    # np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "    enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "    labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(tqdm(f, total=total)):\n",
    "            if total <= i:\n",
    "                print(\"data load early stop\", total, i)\n",
    "                break\n",
    "            data = json.loads(line)\n",
    "            # encoder token\n",
    "            enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "            enc_token += [0] * (n_seq - len(enc_token))\n",
    "            # segment\n",
    "            segment = data[\"segment\"]\n",
    "            segment += [0] * (n_seq - len(segment))\n",
    "            # nsp label\n",
    "            label_nsp = data[\"is_next\"]\n",
    "            # mlm label\n",
    "            mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "            mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "            label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "            label_mlm[mask_idx] = mask_label\n",
    "\n",
    "            assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "            enc_tokens[i] = enc_token\n",
    "            segments[i] = segment\n",
    "            labels_nsp[i] = label_nsp\n",
    "            labels_mlm[i] = label_mlm\n",
    "\n",
    "    return (enc_tokens, segments), (labels_nsp, labels_mlm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4355a2",
   "metadata": {},
   "source": [
    "## 데이터셋 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "390cdb88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7107dd1434414c1da293c818a0f4a15d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/918189 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_544/2049745891.py:42: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
      "/tmp/ipykernel_544/2049745891.py:43: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
      "/tmp/ipykernel_544/2049745891.py:44: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n"
     ]
    }
   ],
   "source": [
    "pre_train_inputs, pre_train_labels = load_pre_train_data(vocab, pretrain_json_path, 128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a7b3be",
   "metadata": {},
   "source": [
    "# BERT 모델 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f93faa",
   "metadata": {},
   "source": [
    "## 마스크 생성 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a05efe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    pad mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: pad mask (pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_ahead_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    ahead mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: ahead and pad mask (ahead or pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    n_seq = tf.shape(tokens)[1]\n",
    "    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
    "    ahead_mask = tf.expand_dims(ahead_mask, axis=0)\n",
    "    pad_mask = get_pad_mask(tokens, i_pad)\n",
    "    mask = tf.maximum(ahead_mask, pad_mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb746f0",
   "metadata": {},
   "source": [
    "## GELU 활성화 함수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a54d156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(experimental_relax_shapes=True)\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    gelu activation 함수\n",
    "    :param x: 입력 값\n",
    "    :return: gelu activation result\n",
    "    \"\"\"\n",
    "    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715f59bd",
   "metadata": {},
   "source": [
    "## weights 초기화 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8de409c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_initializer(stddev=0.02):\n",
    "    \"\"\"\n",
    "    parameter initializer 생성\n",
    "    :param stddev: 생성할 랜덤 변수의 표준편차\n",
    "    \"\"\"\n",
    "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n",
    "\n",
    "\n",
    "def bias_initializer():\n",
    "    \"\"\"\n",
    "    bias initializer 생성\n",
    "    \"\"\"\n",
    "    return tf.zeros_initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a34e21",
   "metadata": {},
   "source": [
    "## Config 관리 클래스\n",
    "- 학습시 사용하는 하이퍼 파라미터를 JSON 파일에서 불러올 수 있는 파이썬 딕셔너리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b6a33ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    json을 config 형태로 사용하기 위한 Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        \"\"\"\n",
    "        file에서 Config를 생성 함\n",
    "        :param file: filename\n",
    "        \"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977c7500",
   "metadata": {},
   "source": [
    "## Token Embedding 레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d4fcb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Weighed Shaed Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"weight_shared_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.n_vocab = config.n_vocab\n",
    "        self.d_model = config.d_model\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        shared weight 생성\n",
    "        :param input_shape: Tensor Shape (not used)\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"shared_embedding_weight\"):\n",
    "            self.shared_weights = self.add_weight(\n",
    "                \"weights\",\n",
    "                shape=[self.n_vocab, self.d_model],\n",
    "                initializer=kernel_initializer()\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, mode=\"embedding\"):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :param mode: 실행 모드\n",
    "        :return: embedding or linear 실행 결과\n",
    "        \"\"\"\n",
    "        # mode가 embedding일 경우 embedding lookup 실행\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(inputs)\n",
    "        # mode가 linear일 경우 linear 실행\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(inputs)\n",
    "        # mode가 기타일 경우 오류 발생\n",
    "        else:\n",
    "            raise ValueError(f\"mode {mode} is not valid.\")\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        \"\"\"\n",
    "        embedding lookup\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
    "        return embed\n",
    "\n",
    "    def _linear(self, inputs):  # (bs, n_seq, d_model)\n",
    "        \"\"\"\n",
    "        linear 실행\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        n_batch = tf.shape(inputs)[0]\n",
    "        n_seq = tf.shape(inputs)[1]\n",
    "        inputs = tf.reshape(inputs, [-1, self.d_model])  # (bs * n_seq, d_model)\n",
    "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
    "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # (bs, n_seq, n_vocab)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b988e5",
   "metadata": {},
   "source": [
    "## Position Embedding 레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c404713",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"position_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :return embed: position embedding lookup 결과\n",
    "        \"\"\"\n",
    "        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)\n",
    "        embed = self.embedding(position)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4adcb6b",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention 레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbb37bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Scale Dot Product Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
    "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        attn_scale = tf.math.divide(attn_score, scale)\n",
    "        attn_scale -= 1.e9 * attn_mask\n",
    "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
    "        attn_out = tf.matmul(attn_prob, V)\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4232d5d7",
   "metadata": {},
   "source": [
    "## Multi-Head Attention 레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e54b956",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi Head Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"multi_head_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "\n",
    "        # Q, K, V input dense layer\n",
    "        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        # Scale Dot Product Attention class\n",
    "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
    "        # output dense layer\n",
    "        self.W_O = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        # reshape Q, K, V, attn_mask\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n",
    "        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n",
    "        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n",
    "        attn_out_m = self.attention(Q_m, K_m, V_m, attn_mask_m) # (bs, n_head, Q_len, d_head)\n",
    "        # transpose and liner\n",
    "        attn_out_m = tf.transpose(attn_out_m, [0, 2, 1, 3])  # (bs, Q_len, n_head, d_head)\n",
    "        attn_out = tf.reshape(attn_out_m, [batch_size, -1, self.n_head * self.d_head])  # (bs, Q_len, d_model)\n",
    "        attn_out = self.W_O(attn_out) # (bs, Q_len, d_model)\n",
    "\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee36a16",
   "metadata": {},
   "source": [
    "## Position-Wise Feed Forward 레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a3a392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Wise Feed Forward Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"feed_forward\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=gelu, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: inputs\n",
    "        :return ff_val: feed forward 실행 결과\n",
    "        \"\"\"\n",
    "        ff_val = self.W_2(self.W_1(inputs))\n",
    "        return ff_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce4edd0",
   "metadata": {},
   "source": [
    "## Encoder 레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7abd816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder Layer Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"encoder_layer\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        # Multi-Head Self Attention\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        # Feed Forward\n",
    "        self.ffn = PositionWiseFeedForward(config)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    " \n",
    "    def call(self, enc_embed, self_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param enc_embed: enc_embed 또는 이전 EncoderLayer의 출력\n",
    "        :param self_mask: enc_tokens의 pad mask\n",
    "        :return enc_out: EncoderLayer 실행 결과\n",
    "        \"\"\"\n",
    "        # Multi-Head Self Attention\n",
    "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)\n",
    "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))\n",
    "\n",
    "        # Feed Forward\n",
    "        ffn_val = self.ffn(norm1_val)\n",
    "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n",
    "\n",
    "        return enc_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aac81ac",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8cc3811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    BERT Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"bert\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.i_pad = config.i_pad\n",
    "        self.embedding = SharedEmbedding(config)\n",
    "        self.position = PositionEmbedding(config)\n",
    "        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "        \n",
    "        self.encoder_layers = [EncoderLayer(config, name=f\"encoder_layer_{i}\") for i in range(config.n_layer)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: (enc_tokens, segments)\n",
    "        :return logits: dec_tokens에 대한 다음 토큰 예측 결과 logits\n",
    "        \"\"\"\n",
    "        enc_tokens, segments = inputs\n",
    "\n",
    "        # 패딩 마스크 생성\n",
    "        enc_self_mask = tf.keras.layers.Lambda(get_pad_mask, output_shape=(1, None), name='enc_self_mask')(enc_tokens, self.i_pad)\n",
    "        \n",
    "        # 입력 데이터 임베딩\n",
    "        enc_embed = self.get_embedding(enc_tokens, segments)\n",
    "\n",
    "        # 인코더 레이어\n",
    "        enc_out = self.dropout(enc_embed)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
    "\n",
    "        # NSP\n",
    "        logits_cls = enc_out[:,0] # [CLS]위치에 해당하는 토큰만 반환\n",
    "        # MLM\n",
    "        logits_lm = self.embedding(enc_out, mode=\"linear\")\n",
    "        return logits_cls, logits_lm\n",
    "    \n",
    "    def get_embedding(self, tokens, segments):\n",
    "        \"\"\"\n",
    "        token embedding, position embedding lookup\n",
    "        :param tokens: 입력 tokens\n",
    "        :param segments: 입력 segments\n",
    "        :return embed: embedding 결과\n",
    "        \"\"\"\n",
    "        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)\n",
    "        embed = self.norm(embed)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b64f2f",
   "metadata": {},
   "source": [
    "## NSP 헤더\n",
    "- NSP 과제 해결을 위한 추가적인 헤더\n",
    "- 출력 노드가 2개\n",
    "    - MLM처럼 SparseCategoricalCrossentropy로 손실값을 계산하기 위함\n",
    "    - binary classification을 class가 2개인 multi-class classification처럼 바꾼 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a931c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PooledOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, n_output, name=\"pooled_output\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(config.d_model, activation=tf.nn.tanh, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.dense2 = tf.keras.layers.Dense(n_output, use_bias=False, activation=tf.nn.softmax, name=\"nsp\", kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    " \n",
    "    def call(self, inputs):\n",
    "        outputs = self.dense1(inputs)\n",
    "        outputs = self.dense2(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e5cbdd",
   "metadata": {},
   "source": [
    "## 모델 생성 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d169d11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_pre_train(config):\n",
    "    enc_tokens = tf.keras.layers.Input((None,), name=\"enc_tokens\")\n",
    "    segments = tf.keras.layers.Input((None,), name=\"segments\")\n",
    "\n",
    "    bert = BERT(config)\n",
    "    logits_cls, logits_lm = bert((enc_tokens, segments))\n",
    "\n",
    "    logits_cls = PooledOutput(config, 2, name=\"pooled_nsp\")(logits_cls)\n",
    "    outputs_nsp = tf.keras.layers.Softmax(name=\"nsp\")(logits_cls)\n",
    "\n",
    "    outputs_mlm = tf.keras.layers.Softmax(name=\"mlm\")(logits_lm)\n",
    "\n",
    "    model = tf.keras.Model(inputs=(enc_tokens, segments), outputs=(outputs_nsp, outputs_mlm))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb612260",
   "metadata": {},
   "source": [
    "# 모델 pretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d32239",
   "metadata": {},
   "source": [
    "## 손실 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cab2bd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    loss 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # loss 계산\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    return loss * 20  # mlm을 더 잘 학습하도록 20배 증가 시킴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c6dd4",
   "metadata": {},
   "source": [
    "## 정확도 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b798bbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    acc 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # 정답 여부 확인\n",
    "    y_pred_class = tf.cast(K.argmax(y_pred, axis=-1), tf.float32)\n",
    "    matches = tf.cast(K.equal(y_true, y_pred_class), tf.float32)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=matches.dtype)\n",
    "    matches *= mask\n",
    "    # 정확도 계산\n",
    "    accuracy = K.sum(matches) / K.maximum(K.sum(mask), 1)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70461c1f",
   "metadata": {},
   "source": [
    "## 학습률 스케쥴러"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55caec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    CosineSchedule Class\n",
    "    \"\"\"\n",
    "    def __init__(self, train_steps=4000, warmup_steps=2000, max_lr=2.5e-4):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param train_steps: 학습 step 총 합\n",
    "        :param warmup_steps: warmup steps\n",
    "        :param max_lr: 최대 learning rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert 0 < warmup_steps < train_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.train_steps = train_steps\n",
    "        self.max_lr = max_lr\n",
    "\n",
    "    def __call__(self, step_num):\n",
    "        \"\"\"\n",
    "        learning rate 계산\n",
    "        :param step_num: 현재 step number\n",
    "        :retrun: 계산된 learning rate\n",
    "        \"\"\"\n",
    "        state = tf.cast(step_num <= self.warmup_steps, tf.float32)\n",
    "        lr1 = tf.cast(step_num, tf.float32) / self.warmup_steps\n",
    "        progress = tf.cast(step_num - self.warmup_steps, tf.float32) / max(1, self.train_steps - self.warmup_steps)\n",
    "        lr2 = 0.5 * (1.0 + tf.math.cos(math.pi * progress))\n",
    "        return (state * lr1 + (1 - state) * lr2) * self.max_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abde7cb",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8218669a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config({\"d_model\": 128, \n",
    "                 \"n_head\": 4, \n",
    "                 \"d_head\": 32, \n",
    "                 \"dropout\": 0.1, \n",
    "                 \"d_ff\": 1024, \n",
    "                 \"layernorm_epsilon\": 0.001, \n",
    "                 \"n_layer\": 2, \n",
    "                 \"n_seq\": 256, \n",
    "                 \"n_vocab\": 0, \n",
    "                 \"i_pad\": 0})\n",
    "\n",
    "config.n_vocab = len(vocab)\n",
    "config.i_pad = vocab.pad_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9ff094",
   "metadata": {},
   "source": [
    "## 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f639012a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_tokens (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segments (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BERT)                     ((None, 128), (None, 1717888     enc_tokens[0][0]                 \n",
      "                                                                 segments[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pooled_nsp (PooledOutput)       (None, 2)            16768       bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "nsp (Softmax)                   (None, 2)            0           pooled_nsp[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mlm (Softmax)                   (None, None, 8007)   0           bert[0][1]                       \n",
      "==================================================================================================\n",
      "Total params: 1,734,656\n",
      "Trainable params: 1,734,656\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pre_train_model = build_model_pre_train(config)\n",
    "pre_train_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1496dbf",
   "metadata": {},
   "source": [
    "## 옵티마이저 설정 및 모델 컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "61b7c175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_steps: 17940\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 512\n",
    "\n",
    "# learning rate scheduler\n",
    "train_steps = math.ceil(len(pre_train_inputs[0]) / batch_size) * epochs\n",
    "print(\"train_steps:\", train_steps)\n",
    "learning_rate = CosineSchedule(train_steps=train_steps, warmup_steps=max(100, train_steps // 10))\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# compile\n",
    "pre_train_model.compile(loss=(tf.keras.losses.sparse_categorical_crossentropy, lm_loss), \n",
    "                        optimizer=optimizer, \n",
    "                        metrics={\"nsp\": \"acc\", \"mlm\": lm_acc})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4e61dc",
   "metadata": {},
   "source": [
    "## 모델 pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "53d0027a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1794/1794 [==============================] - 1015s 564ms/step - loss: 17.4740 - nsp_loss: 0.5778 - mlm_loss: 16.8962 - nsp_acc: 0.6953 - mlm_lm_acc: 0.0779\n",
      "\n",
      "Epoch 00001: mlm_lm_acc improved from -inf to 0.07793, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 2/10\n",
      "1794/1794 [==============================] - 1013s 565ms/step - loss: 15.2131 - nsp_loss: 0.5231 - mlm_loss: 14.6899 - nsp_acc: 0.7503 - mlm_lm_acc: 0.1259\n",
      "\n",
      "Epoch 00002: mlm_lm_acc improved from 0.07793 to 0.12590, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 3/10\n",
      "1794/1794 [==============================] - 1014s 565ms/step - loss: 13.8594 - nsp_loss: 0.5173 - mlm_loss: 13.3421 - nsp_acc: 0.7534 - mlm_lm_acc: 0.1543\n",
      "\n",
      "Epoch 00003: mlm_lm_acc improved from 0.12590 to 0.15433, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 4/10\n",
      "1794/1794 [==============================] - 1015s 566ms/step - loss: 12.4840 - nsp_loss: 0.5144 - mlm_loss: 11.9696 - nsp_acc: 0.7554 - mlm_lm_acc: 0.1865\n",
      "\n",
      "Epoch 00004: mlm_lm_acc improved from 0.15433 to 0.18651, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 5/10\n",
      "1794/1794 [==============================] - 1015s 566ms/step - loss: 11.9727 - nsp_loss: 0.5108 - mlm_loss: 11.4619 - nsp_acc: 0.7590 - mlm_lm_acc: 0.2036\n",
      "\n",
      "Epoch 00005: mlm_lm_acc improved from 0.18651 to 0.20360, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 6/10\n",
      "1794/1794 [==============================] - 1014s 565ms/step - loss: 11.7061 - nsp_loss: 0.5088 - mlm_loss: 11.1974 - nsp_acc: 0.7611 - mlm_lm_acc: 0.2126\n",
      "\n",
      "Epoch 00006: mlm_lm_acc improved from 0.20360 to 0.21255, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 7/10\n",
      "1794/1794 [==============================] - 1014s 565ms/step - loss: 11.5547 - nsp_loss: 0.5074 - mlm_loss: 11.0473 - nsp_acc: 0.7629 - mlm_lm_acc: 0.2176\n",
      "\n",
      "Epoch 00007: mlm_lm_acc improved from 0.21255 to 0.21759, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 8/10\n",
      "1794/1794 [==============================] - 1014s 565ms/step - loss: 11.4673 - nsp_loss: 0.5064 - mlm_loss: 10.9609 - nsp_acc: 0.7643 - mlm_lm_acc: 0.2206\n",
      "\n",
      "Epoch 00008: mlm_lm_acc improved from 0.21759 to 0.22056, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 9/10\n",
      "1794/1794 [==============================] - 1015s 566ms/step - loss: 11.4228 - nsp_loss: 0.5058 - mlm_loss: 10.9169 - nsp_acc: 0.7649 - mlm_lm_acc: 0.2221\n",
      "\n",
      "Epoch 00009: mlm_lm_acc improved from 0.22056 to 0.22209, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 10/10\n",
      "1794/1794 [==============================] - 1014s 565ms/step - loss: 11.4065 - nsp_loss: 0.5056 - mlm_loss: 10.9009 - nsp_acc: 0.7658 - mlm_lm_acc: 0.2227\n",
      "\n",
      "Epoch 00010: mlm_lm_acc improved from 0.22209 to 0.22272, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n"
     ]
    }
   ],
   "source": [
    "# callback\n",
    "save_weights = tf.keras.callbacks.ModelCheckpoint(f\"{model_dir}/bert_pre_train.hdf5\", \n",
    "                                                  monitor=\"mlm_lm_acc\", \n",
    "                                                  verbose=1, save_best_only=True, \n",
    "                                                  mode=\"max\", \n",
    "                                                  save_freq=\"epoch\", \n",
    "                                                  save_weights_only=True)\n",
    "\n",
    "# train\n",
    "history = pre_train_model.fit(pre_train_inputs,   # inputs\n",
    "                              pre_train_labels, # labels\n",
    "                              epochs=epochs,            # epochs\n",
    "                              batch_size=batch_size,    # batch size\n",
    "                              callbacks=[save_weights]  # callback\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "768fa8d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training result\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['nsp_loss'], 'b-', label='nsp_loss')\n",
    "plt.plot(history.history['mlm_loss'], 'r--', label='mlm_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['nsp_acc'], 'g-', label='nsp_acc')\n",
    "plt.plot(history.history['mlm_lm_acc'], 'k--', label='mlm_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5971a843",
   "metadata": {},
   "source": [
    "# 회고\n",
    "- pre-trained 모델을 만들기 위한 방법이 매우 방대하다\n",
    "    - pre-train 이후 specific task를 학습하기 위한 헤더를 붙일 수 있는 모델 구조\n",
    "    - 어떤 specific task든 빠르고 좋은 성능으로 학습할 수 있도록 pre-train하는 방법\n",
    "    - 어떤 task도 모델이 받아내도록 도와주는 입력 데이터 template\n",
    "    - 다양한 task를 가진 데이터셋\n",
    "- pre-trained 모델은 만들어진 걸 쓰는것이 답이다\n",
    "    - 그나마 작은 모델임에도 한 epoch에 16분\n",
    "    - [Gugugo](https://github.com/jwj7140/Gugugo) 개발자가 왜 1epoch 학습후 결과를 올렸는지 이해하게됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91162709",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
