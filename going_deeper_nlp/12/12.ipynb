{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd56b1f6",
   "metadata": {},
   "source": [
    "# 한영 번역기 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a52af8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "import html\n",
    "import re\n",
    "import os\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b722f7bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "import matplotlib.font_manager as fm\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager.findfont(font)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88793ee",
   "metadata": {},
   "source": [
    "# Step 1. 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceac8380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-08-24 06:48:01--  https://github.com/jungyeul/korean-parallel-corpora/raw/master/korean-english-news-v1/korean-english-park.train.tar.gz\n",
      "Resolving github.com (github.com)... 192.30.255.112\n",
      "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/jungyeul/korean-parallel-corpora/master/korean-english-news-v1/korean-english-park.train.tar.gz [following]\n",
      "--2023-08-24 06:48:02--  https://raw.githubusercontent.com/jungyeul/korean-parallel-corpora/master/korean-english-news-v1/korean-english-park.train.tar.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8718893 (8.3M) [application/octet-stream]\n",
      "Saving to: ‘korean-english-park.train.tar.gz’\n",
      "\n",
      "korean-english-park 100%[===================>]   8.31M  --.-KB/s    in 0.09s   \n",
      "\n",
      "2023-08-24 06:48:02 (91.5 MB/s) - ‘korean-english-park.train.tar.gz’ saved [8718893/8718893]\n",
      "\n",
      "--2023-08-24 06:48:03--  https://github.com/jungyeul/korean-parallel-corpora/raw/master/korean-english-news-v1/korean-english-park.test.tar.gz\n",
      "Resolving github.com (github.com)... 192.30.255.112\n",
      "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/jungyeul/korean-parallel-corpora/master/korean-english-news-v1/korean-english-park.test.tar.gz [following]\n",
      "--2023-08-24 06:48:03--  https://raw.githubusercontent.com/jungyeul/korean-parallel-corpora/master/korean-english-news-v1/korean-english-park.test.tar.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 229831 (224K) [application/octet-stream]\n",
      "Saving to: ‘korean-english-park.test.tar.gz’\n",
      "\n",
      "korean-english-park 100%[===================>] 224.44K  --.-KB/s    in 0.02s   \n",
      "\n",
      "2023-08-24 06:48:03 (12.0 MB/s) - ‘korean-english-park.test.tar.gz’ saved [229831/229831]\n",
      "\n",
      "korean-english-park.train.en\n",
      "korean-english-park.train.ko\n",
      "korean-english-park.test.en\n",
      "korean-english-park.test.ko\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/jungyeul/korean-parallel-corpora/raw/master/korean-english-news-v1/korean-english-park.train.tar.gz\n",
    "!wget https://github.com/jungyeul/korean-parallel-corpora/raw/master/korean-english-news-v1/korean-english-park.test.tar.gz\n",
    "\n",
    "!tar -xvf korean-english-park.train.tar.gz\n",
    "!tar -xvf korean-english-park.test.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6a93ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.ipynb\t\t\t korean-english-park.train.en\r\n",
      "korean-english-park.test.en\t korean-english-park.train.ko\r\n",
      "korean-english-park.test.ko\t korean-english-park.train.tar.gz\r\n",
      "korean-english-park.test.tar.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5d555a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총94123줄의 한국어 문장 로드 완료\n"
     ]
    }
   ],
   "source": [
    "kor_corpus = []\n",
    "with open('korean-english-park.train.ko', 'r') as f:\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        kor_corpus.append(line)\n",
    "\n",
    "print('총' + str(len(kor_corpus)) + '줄의 한국어 문장 로드 완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab55b986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총94123줄의 영어 문장 로드 완료\n"
     ]
    }
   ],
   "source": [
    "eng_corpus = []\n",
    "with open('korean-english-park.train.en', 'r') as f:\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        eng_corpus.append(line)\n",
    "\n",
    "print('총' + str(len(eng_corpus)) + '줄의 영어 문장 로드 완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adcc13bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어: 개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"\n",
      "\n",
      "영어 : Much of personal computing is about \"can you top this?\"\n",
      "\n",
      "한국어: 모든 광마우스와 마찬가지 로 이 광마우스도 책상 위에 놓는 마우스 패드를 필요로 하지 않는다.\n",
      "\n",
      "영어 : so a mention a few weeks ago about a rechargeable wireless optical mouse brought in another rechargeable, wireless mouse.\n",
      "\n",
      "한국어: 그러나 이것은 또한 책상도 필요로 하지 않는다.\n",
      "\n",
      "영어 : Like all optical mice, But it also doesn't need a desk.\n",
      "\n",
      "한국어: 79.95달러하는 이 최첨단 무선 광마우스는 허공에서 팔목, 팔, 그외에 어떤 부분이든 그 움직임에따라 커서의 움직임을 조절하는 회전 운동 센서를 사용하고 있다.\n",
      "\n",
      "영어 : uses gyroscopic sensors to control the cursor movement as you move your wrist, arm, whatever through the air.\n",
      "\n",
      "한국어: 정보 관리들은 동남 아시아에서의 선박들에 대한 많은 (테러) 계획들이 실패로 돌아갔음을 밝혔으며, 세계 해상 교역량의 거의 3분의 1을 운송하는 좁은 해로인 말라카 해협이 테러 공격을 당하기 쉽다고 경고하고 있다.\n",
      "\n",
      "영어 : Intelligence officials have revealed a spate of foiled plots on ships in Southeast Asia and are warning that a narrow stretch of water carrying almost one third of the world's maritime trade is vulnerable to a terror attack.\n",
      "\n",
      "한국어: 이 지역에 있는 미국 선박과 상업용 선박들에 대한 알카에다의 (테러) 시도 중 여러 건이 실패했다는 것을 알게 된 후에, 전문가들은 테러 조직이 여전히 세계 경제에 타격을 입히려 한다고 경고하고 있으며, 동남 아시아에 있는 세계 경제의 주요 통로가 위험에 처해 있다고 그들은 생각하고 있다.\n",
      "\n",
      "영어 : After learning of several foiled al Qaeda attempts on U.S. and commercial ships in the area, experts are warning that the terror network still wants to cripple the global economy, the world's economic jugular vein in Southeast Asia is at risk.\n",
      "\n",
      "한국어: 국립 과학 학회가 발표한 새 보고서에따르면, 복잡한 임무를 수행해야 하는 군인들이나 보다 오랜 시간 동안 경계를 늦추지 않고 있기 위해 도움이 필요한 군인들에게 카페인이 반응 시간을 증가시키고 임무 수행 능력을 향상시키는데 도움이 된다고 한다.\n",
      "\n",
      "영어 : Caffeine can help increase reaction time and improve performance for military servicemen who must perform complex tasks or who need help staying alert for longer periods of time, according to a new report by the National Academy of Sciences.\n",
      "\n",
      "한국어: 이 보고서에따르면, \"특히, 군사 작전에서 생사가 걸린 상황이 될 수도 있는 반응 속도와 시각 및 청각의 경계 상태를 유지시키기 위해 카페인이 사용될 수도 있다.\" 고 한다.\n",
      "\n",
      "영어 : \"Specifically, it can be used in maintaining speed of reactions and visual and auditory vigilance, which in military operations could be a life or death situation,\" according to the report.\n",
      "\n",
      "한국어: \"결정적인 순간에 그들의 능력을 증가시켜 줄 그 무엇이 매우 중요합니다.\"\n",
      "\n",
      "영어 : \"Something that will boost their capabilities at crucial moments is very important.\"\n",
      "\n",
      "한국어: 연구가들이 이미 커피 대체품으로서 음식 대용 과자나 껌에 카페인을 첨가하는 방법을 연구하고 있다고 Archibald는 말했다.\n",
      "\n",
      "영어 : Researchers are already exploring ways to put caffeine in nutrition bars or chewing gum as alternatives to coffee, Archibald said.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (kor, eng) in enumerate(zip(kor_corpus, eng_corpus)):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print('한국어:', kor)\n",
    "    print('영어 :', eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "181b5391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유시프의 어머니인 자이네브는 이에 대해 “당연하다”고 답변했다.\n",
      "\n",
      "The reply: \"Of course,\" said Youssif's mother, Zainab.\n",
      "\n",
      "이 재단에서 유시프 가족의 여행비용과 유시프의 치료비 전액을 부담하게 된다.\n",
      "\n",
      "For the first time in a long time, the family laughed out of pure joy. See Youssif play with his new toys &raquo;\n",
      "\n",
      "CNN은 TV와 온라인을 통해 지난달 22일 유시프의 안타까운 사연을 보도했다.\n",
      "\n",
      "Standing on the apartment's balcony, Youssif's father turned to Barbara Friedman, executive director of the Children's Burn Foundation.\n",
      "\n",
      "유시프의 가족은 아이의 치료를 위해 외국으로 떠날지 아니면 이라크에 남을지 결정을 내려야 했다.\n",
      "\n",
      "Specifically, the family had to make a decision on whether to leave their homeland or stay inside Iraq for treatment.\n",
      "\n",
      "그러나 가족은 유시프의 장래를 위해 미국행을 고려할 수 밖에 없었다.\n",
      "\n",
      "But the family has decided Youssif should seek treatment in the United States.\n",
      "\n",
      "유시프의 아버지는 아들의 미국행이 결정됐다는 소식에 “행복한 마음으로 비행기에 오를 수 있을 것 같다”며 희망 섞인 발언을 했다.\n",
      "\n",
      "Youssif's father was also cheered by the news.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for kor, eng in zip(kor_corpus, eng_corpus):\n",
    "    if '유시프의' in kor:\n",
    "        print(kor)\n",
    "        print(eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e07c9e3",
   "metadata": {},
   "source": [
    "원본 파일의 한국어와 영어 문장 쌍이 잘 맞지 않는 것으로 보아 모델의 학습 결과가 낮을 것으로 예상"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cffd788",
   "metadata": {},
   "source": [
    "# Step 2. 데이터 정제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda716ce",
   "metadata": {},
   "source": [
    "## 중복 데이터 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13ca8635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복 데이터를 전부 제거한 후 샘플 수: 78968\n"
     ]
    }
   ],
   "source": [
    "cleaned_corpus = list(set(zip(kor_corpus, eng_corpus)))\n",
    "print(\"중복 데이터를 전부 제거한 후 샘플 수:\", len(cleaned_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41cb06bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어 문장: 75577\n",
      "영어 문장: 75577\n"
     ]
    }
   ],
   "source": [
    "kor_corpus = []\n",
    "eng_corpus = []\n",
    "\n",
    "# 한국어와 영어 문장을 다시 분리\n",
    "for kor, eng in cleaned_corpus:\n",
    "    # 학습 속도를 위해 40 단어를 초과하는 문장 제거\n",
    "    if len(kor.split()) > 40 or len(eng.split()) > 40:\n",
    "        continue\n",
    "        \n",
    "    kor_corpus.append(kor)\n",
    "    eng_corpus.append(eng)\n",
    "    \n",
    "print('한국어 문장:', len(kor_corpus))\n",
    "print('영어 문장:', len(eng_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d2748e",
   "metadata": {},
   "source": [
    "## 텍스트 전처리 \n",
    "- 영어 소문자화\n",
    "- html 엔티티 제거\n",
    "    - html 엔티티를 특수문자로 인코딩\n",
    "- 문장부호 단일 토큰화\n",
    "- 긴 공백 줄이기\n",
    "- 한글, 영문자, 숫자, 문장부호를 제외한 모든 글자 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a7e18bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence, s_token=False, e_token=False):\n",
    "    sentence = sentence.lower().strip() # 영어 소문자화\n",
    "    sentence = html.unescape(sentence)  # html 엔티티 인코딩\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence) # 문장부호 분리\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)       # 긴 공백 줄이기\n",
    "    sentence = re.sub(r\"[^ㄱ-ㅎ가-힣a-zA-Z0-9?.!,]+\", \" \", sentence) # 불필요한 글자 제거\n",
    "\n",
    "    sentence = sentence.strip() # 문장의 양쪽 공백 제거\n",
    "\n",
    "    if s_token:\n",
    "        sentence = '<start> ' + sentence\n",
    "\n",
    "    if e_token:\n",
    "        sentence += ' <end>'\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6aa6d11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 전 한국어: 한편 아시아 주식시장은 18일 미 행정부의 경기부양 조치를 앞두고 일시적으로 상승한 바 있다.\n",
      "\n",
      "전처리 후 한국어: 한편 아시아 주식시장은 18일 미 행정부의 경기부양 조치를 앞두고 일시적으로 상승한 바 있다 .\n",
      "전처리 전 영어 : On Friday major Asian markets recovered from early plunges on the hopes that Washington will soon propose measures to keep the U.S. economy from sliding into a recession.\n",
      "\n",
      "전처리 후 영어 : <start> on friday major asian markets recovered from early plunges on the hopes that washington will soon propose measures to keep the u . s . economy from sliding into a recession . <end>\n",
      "--------------------------------------------------------------------------------\n",
      "전처리 전 한국어: lose control over ~을 통제하지 못하다\n",
      "\n",
      "전처리 후 한국어: lose control over 을 통제하지 못하다\n",
      "전처리 전 영어 : “ They need to know that the minute they hit that Send button, they not only lose control over where these photos go next, in some cases they lose control over their own future.”\n",
      "\n",
      "전처리 후 영어 : <start> they need to know that the minute they hit that send button , they not only lose control over where these photos go next , in some cases they lose control over their own future . <end>\n",
      "--------------------------------------------------------------------------------\n",
      "전처리 전 한국어: 프로젝트 동료 마틴 반 나이에롭은 \"이 술은 액상이 아니기때문에 16세 이하 소비자들에게 이 제품을 팔 수 있다\"고 확신했다.\n",
      "\n",
      "전처리 후 한국어: 프로젝트 동료 마틴 반 나이에롭은 이 술은 액상이 아니기때문에 16세 이하 소비자들에게 이 제품을 팔 수 있다 고 확신했다 .\n",
      "전처리 전 영어 : \"I think they're unsure about their own regulations to be honest,\" director Stephen Pearson said.\n",
      "\n",
      "전처리 후 영어 : <start> i think they re unsure about their own regulations to be honest , director stephen pearson said . <end>\n",
      "--------------------------------------------------------------------------------\n",
      "전처리 전 한국어: 이는 미국의 항구나 공항으로 입국하는 미국 시민들은 여권이나 여권카드, WHTI 준수 증명서를 소지해야만 한다는 것을 의미한다.\n",
      "\n",
      "전처리 후 한국어: 이는 미국의 항구나 공항으로 입국하는 미국 시민들은 여권이나 여권카드 , whti 준수 증명서를 소지해야만 한다는 것을 의미한다 .\n",
      "전처리 전 영어 : That means U.S. citizens entering the United States at sea or land ports of entry must either have a passport, passport card or WHTI-compliant document.\n",
      "\n",
      "전처리 후 영어 : <start> that means u . s . citizens entering the united states at sea or land ports of entry must either have a passport , passport card or whti compliant document . <end>\n",
      "--------------------------------------------------------------------------------\n",
      "전처리 전 한국어: 라이스 장관은 다음달 일본에서 열리는 G-8(선진 8개국) 정상회담을 위해 아시아 순방일정을 앞두고 이번 연설을 가졌다.\n",
      "\n",
      "전처리 후 한국어: 라이스 장관은 다음달 일본에서 열리는 g 8 선진 8개국 정상회담을 위해 아시아 순방일정을 앞두고 이번 연설을 가졌다 .\n",
      "전처리 전 영어 : Rice spoke in advance of her upcoming trip to Asia where she will be attending a meeting of G8 foreign ministers and meeting with her Asian counterparts .\n",
      "\n",
      "전처리 후 영어 : <start> rice spoke in advance of her upcoming trip to asia where she will be attending a meeting of g8 foreign ministers and meeting with her asian counterparts . <end>\n",
      "--------------------------------------------------------------------------------\n",
      "전처리 전 한국어: 스튜어트는 “세부 묘사가 너무 신비롭고 아름다워 분명 그 안에 어떤 메시지가 담겨있을 것이라고 생각했다’고 전했다.\n",
      "\n",
      "전처리 후 한국어: 스튜어트는 세부 묘사가 너무 신비롭고 아름다워 분명 그 안에 어떤 메시지가 담겨있을 것이라고 생각했다 고 전했다 .\n",
      "전처리 전 영어 : \"They are of such exquisite detail and so beautiful that we thought there must be a message here,\" he told Reuters.\n",
      "\n",
      "전처리 후 영어 : <start> they are of such exquisite detail and so beautiful that we thought there must be a message here , he told reuters . <end>\n",
      "--------------------------------------------------------------------------------\n",
      "전처리 전 한국어: *만일 삼킨 물질이 보이면 그것을 따라 빼내면 된다.\n",
      "\n",
      "전처리 후 한국어: 만일 삼킨 물질이 보이면 그것을 따라 빼내면 된다 .\n",
      "전처리 전 영어 : If you can see the object, you can go in after it.\n",
      "\n",
      "전처리 후 영어 : <start> if you can see the object , you can go in after it . <end>\n",
      "--------------------------------------------------------------------------------\n",
      "전처리 전 한국어: 마이크로소프트의 xbox 360 총괄 임원인 로비 바흐는 이번 발표가 소니의 새로운 전략 움직임과 관련이 없다고 전했다.\n",
      "\n",
      "전처리 후 한국어: 마이크로소프트의 xbox 360 총괄 임원인 로비 바흐는 이번 발표가 소니의 새로운 전략 움직임과 관련이 없다고 전했다 .\n",
      "전처리 전 영어 : Robbie Bach, president of Microsoft's entertainment and devices division, said the timing of the announcement about the charge for the quarter ending in June and a new extended warranty were unrelated to any potential move by Sony.\n",
      "\n",
      "전처리 후 영어 : <start> robbie bach , president of microsoft s entertainment and devices division , said the timing of the announcement about the charge for the quarter ending in june and a new extended warranty were unrelated to any potential move by sony . <end>\n",
      "--------------------------------------------------------------------------------\n",
      "전처리 전 한국어: 하지만, 요즘의 한국에는 일반적인 경제학 이론을 적용할 수 없는 것 같다.\n",
      "\n",
      "전처리 후 한국어: 하지만 , 요즘의 한국에는 일반적인 경제학 이론을 적용할 수 없는 것 같다 .\n",
      "전처리 전 영어 : But these days the general economic theory seems to be not applicable in Korea.\n",
      "\n",
      "전처리 후 영어 : <start> but these days the general economic theory seems to be not applicable in korea . <end>\n",
      "--------------------------------------------------------------------------------\n",
      "전처리 전 한국어: EPL이 해외경기를 추진하고 있는 국가들은 대부분 반대하는 입장이어서 프리미어리그 구단들의 입지가 계속 줄어들고 있다.\n",
      "\n",
      "전처리 후 한국어: epl이 해외경기를 추진하고 있는 국가들은 대부분 반대하는 입장이어서 프리미어리그 구단들의 입지가 계속 줄어들고 있다 .\n",
      "전처리 전 영어 : \"Those that are richer than the others, they have more responsibility and what the Premier League is trying to do is contrary to this responsibility.\n",
      "\n",
      "전처리 후 영어 : <start> those that are richer than the others , they have more responsibility and what the premier league is trying to do is contrary to this responsibility . <end>\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "kor_sentences = list(map(preprocess_sentence, kor_corpus))\n",
    "eng_sentences = list(map(lambda s: preprocess_sentence(s, s_token=True, e_token=True), \n",
    "                                  eng_corpus))\n",
    "\n",
    "for i, (kor, kor_pre, eng, eng_pre) in enumerate(zip(kor_corpus, \n",
    "                                                     kor_sentences, \n",
    "                                                     eng_corpus,\n",
    "                                                     eng_sentences)):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print('전처리 전 한국어:', kor)\n",
    "    print('전처리 후 한국어:', kor_pre)\n",
    "    \n",
    "    print('전처리 전 영어 :', eng)\n",
    "    print('전처리 후 영어 :', eng_pre)\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f1e157",
   "metadata": {},
   "source": [
    "# Step 3. 데이터 토큰화\n",
    "## 한국어 문장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d30dbe55",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = Mecab()\n",
    "kor_splited = []\n",
    "for sentence in kor_sentences:\n",
    "    kor_splited.append(mecab.morphs(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8133aa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 30000\n",
    "\n",
    "kor_tokenizer = keras.preprocessing.text.Tokenizer(\n",
    "    num_words=vocab_size,\n",
    ")\n",
    "\n",
    "kor_tokenizer.fit_on_texts(kor_splited)\n",
    "kor_tensor = kor_tokenizer.texts_to_sequences(kor_splited)\n",
    "kor_sequences = keras.preprocessing.sequence.pad_sequences(kor_tensor, maxlen=85, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5304303b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 112,  557,  814,  215,    8,  390,   25,   83,  977,    6,  164,\n",
       "       5272,  283,    9,  987,   12, 3361,   31,   20,  474,   18,  255,\n",
       "         13,    2,    1,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kor_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "690e6e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['한편 아시아 주식 시장 은 18 일 미 행정부 의 경기 부양 조치 를 앞두 고 일시 적 으로 상승 한 바 있 다 .']\n"
     ]
    }
   ],
   "source": [
    "print(kor_tokenizer.sequences_to_texts(kor_sequences[0:1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97737bbc",
   "metadata": {},
   "source": [
    "## 영어 문장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25d6e35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_splited = [eng.split() for eng in eng_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1344869a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokenizer = keras.preprocessing.text.Tokenizer(\n",
    "    num_words=vocab_size,\n",
    ")\n",
    "\n",
    "eng_tokenizer.fit_on_texts(eng_splited)\n",
    "eng_tensor = eng_tokenizer.texts_to_sequences(eng_splited)\n",
    "eng_sequences = keras.preprocessing.sequence.pad_sequences(eng_tensor, maxlen=59, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f190602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    4,    14,   240,   300,   902,   763,  2652,    23,   227,\n",
       "       13224,    14,     1,  1092,    15,   213,    33,   644,  7836,\n",
       "         954,     6,   579,     1,    47,     2,    11,     2,   312,\n",
       "          23,  9423,    74,     8,  1787,     2,     5,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01c44f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start> on friday major asian markets recovered from early plunges on the hopes that washington will soon propose measures to keep the u . s . economy from sliding into a recession . <end>']\n"
     ]
    }
   ],
   "source": [
    "print(eng_tokenizer.sequences_to_texts(eng_sequences[0:1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaec3c2",
   "metadata": {},
   "source": [
    "# Step 4. 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab5981b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.w_encoder = layers.Dense(units)\n",
    "        self.w_decoder = layers.Dense(units)\n",
    "        self.w_combine = layers.Dense(1)\n",
    "\n",
    "    def call(self, H_encoder, H_decoder):\n",
    "        H_encoder = self.w_encoder(H_encoder)\n",
    "\n",
    "        H_decoder = tf.expand_dims(H_decoder, axis=1)\n",
    "        H_decoder = self.w_decoder(H_decoder)\n",
    "        \n",
    "        score = self.w_combine(tf.nn.tanh(H_encoder + H_decoder))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vector = attention_weights * H_encoder\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0df1941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(keras.Model):\n",
    "    def __init__(self, vocab_size, embed_dim, enc_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = layers.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = layers.GRU(enc_dim,\n",
    "                              return_sequences=True) # 어텐선 점수 계산을 위해 각 스탭별 hidden state를 반환\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output = self.gru(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66168b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(keras.Model):\n",
    "    def __init__(self, vocab_size, embed_dim, dec_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = layers.Embedding(vocab_size, embed_dim)\n",
    "        self.attention = BahdanauAttention(dec_dim)\n",
    "        self.gru = layers.GRU(dec_dim, \n",
    "                              return_sequences=True,\n",
    "                              return_state=True)\n",
    "        self.fc = layers.Dense(vocab_size)\n",
    "    \n",
    "    def call(self, x, h_decoder, h_encoder):\n",
    "        x = self.embedding(x)\n",
    "        context_v, attn = self.attention(h_encoder, h_decoder)\n",
    "        x = tf.concat([tf.expand_dims(context_v, axis=1), x],\n",
    "                      axis=-1)\n",
    "        x, h_decoder = self.gru(x)\n",
    "        x = self.fc(x)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "        return x, h_decoder, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d656e143",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "units = 64\n",
    "\n",
    "encoder = Encoder(vocab_size, embed_dim, units)\n",
    "decoder = Decoder(vocab_size, embed_dim, units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531dccc0",
   "metadata": {},
   "source": [
    "# Step 5: 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3921b7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 옵티마이저\n",
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "# 손실함수\n",
    "loss_object = keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fbd6c6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(src, tgt, encoder, decoder, optimizer, dec_tok):\n",
    "    bsz = src.shape[0] # 배치사이즈\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 인코더\n",
    "        src = tf.reverse(src, [-1])\n",
    "        enc_out = encoder(src)\n",
    "        h_dec = enc_out[:, -1] # 마지막 hidden state를 디코더의 첫 hidden_state로 사용\n",
    "\n",
    "        # 디코더\n",
    "        # 디코더가 생성한 시퀀스를 저장할 배열\n",
    "        dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1)\n",
    "\n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            # 다음 단어 예측\n",
    "            pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "            # 손실값 계산\n",
    "            loss += loss_function(tgt[:, t], pred)\n",
    "            # 강제 교수를 위해 디코더의 입력 시퀀스를 정답 시퀀스에서 가져옴\n",
    "            dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "\n",
    "    # 평균 손실 값 계산\n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "\n",
    "    # 옵티마이저로 가중치 업데이트\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a587130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(src, encoder, decoder):\n",
    "    # 입력 데이터 전처리\n",
    "    src = preprocess_sentence(src)\n",
    "    src = mecab.morphs(src)\n",
    "    src = kor_tokenizer.texts_to_sequences([src])\n",
    "    src = keras.preprocessing.sequence.pad_sequences(src, maxlen=85, padding='post')\n",
    "    \n",
    "    # 인코더\n",
    "    src = tf.reverse(src, [-1])\n",
    "    enc_out = encoder(src)\n",
    "    dec_hidden = enc_out[:,-1]\n",
    "    dec_input = tf.expand_dims([eng_tokenizer.word_index['<start>']], 0)\n",
    "    \n",
    "    output = np.zeros((59,))\n",
    "    \n",
    "    for t in range(59):\n",
    "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_out)\n",
    "\n",
    "        # 디코더가 예측한 단어 글자화\n",
    "        predicted_id = tf.argmax(tf.math.softmax(predictions, axis=-1)[0]).numpy()\n",
    "\n",
    "        # 종료 토큰 발견시 즉시 종료\n",
    "        if predicted_id == 0 or eng_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            break\n",
    "        \n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "        output[t] = predicted_id\n",
    "\n",
    "    return eng_tokenizer.sequences_to_texts(output.reshape((1, -1)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "14577aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|██████████| 148/148 [01:13<00:00,  2.00it/s, Loss 1.9392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61 Result\n",
      "오바마는 대통령이다.\n",
      "=> obama s office said he said .\n",
      "\n",
      "시민들은 도시 속에 산다.\n",
      "=> he said .\n",
      "\n",
      "커피는 필요 없다.\n",
      "=> the man was not to be in the first time , the police chief executive director of the people were not to the first time , the police chief executive director of the people were not to the first time , the police chief executive director of the people were not to the first time , the police chief\n",
      "\n",
      "일곱 명의 사망자가 발생했다.\n",
      "=> according to the world s first time , was not to the world s first time , was not to the world s first time , was not to the world s first time , was not to the world s first time , was not to the world s first time , was not to the world s\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|██████████| 148/148 [01:13<00:00,  2.01it/s, Loss 1.9330]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62 Result\n",
      "오바마는 대통령이다.\n",
      "=> obama s office said he said .\n",
      "\n",
      "시민들은 도시 속에 산다.\n",
      "=> he said .\n",
      "\n",
      "커피는 필요 없다.\n",
      "=> the new york times reports said .\n",
      "\n",
      "일곱 명의 사망자가 발생했다.\n",
      "=> according to the world s largest , 000 people were killed in the world s largest , 000 people were killed in the world s largest , 000 people were killed in the world s largest , 000 people were killed in the world s largest , 000 people were killed in the world s largest , 000 people\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|██████████| 148/148 [01:13<00:00,  2.01it/s, Loss 1.9270]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63 Result\n",
      "오바마는 대통령이다.\n",
      "=> obama s office said he said .\n",
      "\n",
      "시민들은 도시 속에 산다.\n",
      "=> he said .\n",
      "\n",
      "커피는 필요 없다.\n",
      "=> the new york times reports said .\n",
      "\n",
      "일곱 명의 사망자가 발생했다.\n",
      "=> cnn a group of the government is the first time , the police chief executive director of the people were killed and that the government is the first time , the police chief executive director of the people were killed and that the government is the first time , the police chief executive director of the people were killed\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|██████████| 148/148 [01:13<00:00,  2.01it/s, Loss 1.9222]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 Result\n",
      "오바마는 대통령이다.\n",
      "=> obama s office said .\n",
      "\n",
      "시민들은 도시 속에 산다.\n",
      "=> he said .\n",
      "\n",
      "커피는 필요 없다.\n",
      "=> the ap s is the most important thing is not to be in the first time , the police department said .\n",
      "\n",
      "일곱 명의 사망자가 발생했다.\n",
      "=> according to the world s largest economy , he said .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65: 100%|██████████| 148/148 [01:13<00:00,  2.01it/s, Loss 1.9181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 Result\n",
      "오바마는 대통령이다.\n",
      "=> obama is the first time .\n",
      "\n",
      "시민들은 도시 속에 산다.\n",
      "=> he said .\n",
      "\n",
      "커피는 필요 없다.\n",
      "=> the new york times reports said .\n",
      "\n",
      "일곱 명의 사망자가 발생했다.\n",
      "=> according to the world s largest church , and the u . s . , and the u . s . , and the u . s . , and the u . s . , and the u . s . , and the u . s . , and the u . s . , and the\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66: 100%|██████████| 148/148 [01:13<00:00,  2.01it/s, Loss 1.9128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66 Result\n",
      "오바마는 대통령이다.\n",
      "=> obama s office said he said .\n",
      "\n",
      "시민들은 도시 속에 산다.\n",
      "=> he said .\n",
      "\n",
      "커피는 필요 없다.\n",
      "=> the number of the number of the people were to be in the first time , the police department said .\n",
      "\n",
      "일곱 명의 사망자가 발생했다.\n",
      "=> although the united states is the first time , the president is the first time , the president is the first time , the president is the first time , the president is the first time , the president is the first time , the president is the first time , the president is the first time , the\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67: 100%|██████████| 148/148 [01:13<00:00,  2.01it/s, Loss 1.9069]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67 Result\n",
      "오바마는 대통령이다.\n",
      "=> obama s office said he said .\n",
      "\n",
      "시민들은 도시 속에 산다.\n",
      "=> he said .\n",
      "\n",
      "커피는 필요 없다.\n",
      "=> the new york times reports said .\n",
      "\n",
      "일곱 명의 사망자가 발생했다.\n",
      "=> according to the world s largest economy , he said .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68: 100%|██████████| 148/148 [01:13<00:00,  2.01it/s, Loss 1.9015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68 Result\n",
      "오바마는 대통령이다.\n",
      "=> obama s office said he said .\n",
      "\n",
      "시민들은 도시 속에 산다.\n",
      "=> he said .\n",
      "\n",
      "커피는 필요 없다.\n",
      "=> the number of the number of the number of the number of the first time , is a lot of dollars .\n",
      "\n",
      "일곱 명의 사망자가 발생했다.\n",
      "=> although the u . s . , who was not to the world s largest economy , he said .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69: 100%|██████████| 148/148 [01:13<00:00,  2.01it/s, Loss 1.8968]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69 Result\n",
      "오바마는 대통령이다.\n",
      "=> obama s office said he said .\n",
      "\n",
      "시민들은 도시 속에 산다.\n",
      "=> he said .\n",
      "\n",
      "커피는 필요 없다.\n",
      "=> the agency said .\n",
      "\n",
      "일곱 명의 사망자가 발생했다.\n",
      "=> according to the world s largest economy , he said .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70: 100%|██████████| 148/148 [01:13<00:00,  2.01it/s, Loss 1.8915]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70 Result\n",
      "오바마는 대통령이다.\n",
      "=> obama s office said he said .\n",
      "\n",
      "시민들은 도시 속에 산다.\n",
      "=> he said .\n",
      "\n",
      "커피는 필요 없다.\n",
      "=> the number of the number of the people were not to be in the first time , the police chief executive of the people were not to be in the first time , the police chief executive of the people were not to be in the first time , the police chief executive of the people were not to\n",
      "\n",
      "일곱 명의 사망자가 발생했다.\n",
      "=> although the u . s . , who was not to the world s largest economy , he said .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71: 100%|██████████| 148/148 [01:13<00:00,  2.01it/s, Loss 1.8865]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71 Result\n",
      "오바마는 대통령이다.\n",
      "=> obama s office said he said .\n",
      "\n",
      "시민들은 도시 속에 산다.\n",
      "=> he said .\n",
      "\n",
      "커피는 필요 없다.\n",
      "=> the ap s is the most important thing is a lot of dollars .\n",
      "\n",
      "일곱 명의 사망자가 발생했다.\n",
      "=> cnn a group of the government is the first time , the police chief of the government is the first time , the police chief of the government is the first time , the police chief of the government is the first time , the police chief of the government is the first time , the police chief of\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72: 100%|██████████| 148/148 [01:13<00:00,  2.01it/s, Loss 1.8828]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72 Result\n",
      "오바마는 대통령이다.\n",
      "=> obama s office said he said .\n",
      "\n",
      "시민들은 도시 속에 산다.\n",
      "=> he said .\n",
      "\n",
      "커피는 필요 없다.\n",
      "=> the man was not to be in the first time , the police chief executive of the number of the first time , is a lot of dollars .\n",
      "\n",
      "일곱 명의 사망자가 발생했다.\n",
      "=> cnn a man who was not to the world s largest economy , but the number of the company said .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73: 100%|██████████| 148/148 [01:13<00:00,  2.01it/s, Loss 1.8783]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73 Result\n",
      "오바마는 대통령이다.\n",
      "=> obama s office said he said .\n",
      "\n",
      "시민들은 도시 속에 산다.\n",
      "=> he said .\n",
      "\n",
      "커피는 필요 없다.\n",
      "=> the new york times reports said .\n",
      "\n",
      "일곱 명의 사망자가 발생했다.\n",
      "=> although the u . s . , who was not to the world s largest economy , he said .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 148/148 [01:13<00:00,  2.00it/s, Loss 1.8741]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74 Result\n",
      "오바마는 대통령이다.\n",
      "=> obama s office said he said .\n",
      "\n",
      "시민들은 도시 속에 산다.\n",
      "=> he said .\n",
      "\n",
      "커피는 필요 없다.\n",
      "=> the number of the number of the first time , is a lot of dollars .\n",
      "\n",
      "일곱 명의 사망자가 발생했다.\n",
      "=> cnn a group of the government is the first time , the president is the first time , the president is the first time , the president is the first time , the president is the first time , the president is the first time , the president is the first time , the president is the first time\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75: 100%|██████████| 148/148 [01:13<00:00,  2.01it/s, Loss 1.8693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 Result\n",
      "오바마는 대통령이다.\n",
      "=> obama s office said he said .\n",
      "\n",
      "시민들은 도시 속에 산다.\n",
      "=> it s .\n",
      "\n",
      "커피는 필요 없다.\n",
      "=> the agency said .\n",
      "\n",
      "일곱 명의 사망자가 발생했다.\n",
      "=> cnn a man who was not to the world s largest economy , he said .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76: 100%|██████████| 148/148 [01:13<00:00,  2.01it/s, Loss 1.8641]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76 Result\n",
      "오바마는 대통령이다.\n",
      "=> obama s office said he said .\n",
      "\n",
      "시민들은 도시 속에 산다.\n",
      "=> it s .\n",
      "\n",
      "커피는 필요 없다.\n",
      "=> the new york times reports said .\n",
      "\n",
      "일곱 명의 사망자가 발생했다.\n",
      "=> although the u . s . , who was not to the world s largest economy , he said .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77: 100%|██████████| 148/148 [01:13<00:00,  2.01it/s, Loss 1.8609]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77 Result\n",
      "오바마는 대통령이다.\n",
      "=> obama s office said he said .\n",
      "\n",
      "시민들은 도시 속에 산다.\n",
      "=> he said .\n",
      "\n",
      "커피는 필요 없다.\n",
      "=> the number of the number of people have been in the most important thing to be in the most important thing to be in the first time , the spokesman said .\n",
      "\n",
      "일곱 명의 사망자가 발생했다.\n",
      "=> cnn a man who was arrested in the world s largest economy , he said .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78: 100%|██████████| 148/148 [01:13<00:00,  2.01it/s, Loss 1.8570]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78 Result\n",
      "오바마는 대통령이다.\n",
      "=> obama s office said he said .\n",
      "\n",
      "시민들은 도시 속에 산다.\n",
      "=> he said .\n",
      "\n",
      "커피는 필요 없다.\n",
      "=> the agency said .\n",
      "\n",
      "일곱 명의 사망자가 발생했다.\n",
      "=> cnn a group of the company said .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79: 100%|██████████| 148/148 [01:13<00:00,  2.01it/s, Loss 1.8529]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79 Result\n",
      "오바마는 대통령이다.\n",
      "=> obama s office said he said .\n",
      "\n",
      "시민들은 도시 속에 산다.\n",
      "=> it s .\n",
      "\n",
      "커피는 필요 없다.\n",
      "=> the number of the number of the number of the people were to be in the first time , the spokesman said the number of the people were to be in the first time , the spokesman said the number of the people were to be in the first time , the spokesman said the number of the people\n",
      "\n",
      "일곱 명의 사망자가 발생했다.\n",
      "=> cnn a group of the company said .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80: 100%|██████████| 148/148 [01:13<00:00,  2.00it/s, Loss 1.8488]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80 Result\n",
      "오바마는 대통령이다.\n",
      "=> obama s office said he said .\n",
      "\n",
      "시민들은 도시 속에 산다.\n",
      "=> he said .\n",
      "\n",
      "커피는 필요 없다.\n",
      "=> the woman was not to be in the most important thing is a lot of dollars .\n",
      "\n",
      "일곱 명의 사망자가 발생했다.\n",
      "=> although the u . s . , who was not to the world s largest economy , he said .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81: 100%|██████████| 148/148 [01:13<00:00,  2.01it/s, Loss 1.8462]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81 Result\n",
      "오바마는 대통령이다.\n",
      "=> obama s office said he said .\n",
      "\n",
      "시민들은 도시 속에 산다.\n",
      "=> he was not to be in the first time , is the people .\n",
      "\n",
      "커피는 필요 없다.\n",
      "=> the agency said .\n",
      "\n",
      "일곱 명의 사망자가 발생했다.\n",
      "=> cnn a group of the company said .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82: 100%|██████████| 148/148 [01:13<00:00,  2.00it/s, Loss 1.8412]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82 Result\n",
      "오바마는 대통령이다.\n",
      "=> obama s office said he said .\n",
      "\n",
      "시민들은 도시 속에 산다.\n",
      "=> he said .\n",
      "\n",
      "커피는 필요 없다.\n",
      "=> the agency said .\n",
      "\n",
      "일곱 명의 사망자가 발생했다.\n",
      "=> cnn a group of the u . s . , was killed , and the u . s . , was killed , and the u . s . , was killed , and the u . s . , was killed , and the u . s . , was killed , and the u . s .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83: 100%|██████████| 148/148 [01:13<00:00,  2.01it/s, Loss 1.8372]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83 Result\n",
      "오바마는 대통령이다.\n",
      "=> obama is the first time .\n",
      "\n",
      "시민들은 도시 속에 산다.\n",
      "=> he was not to be in the same time , but the company said .\n",
      "\n",
      "커피는 필요 없다.\n",
      "=> the man was not to be in the most important thing is not to be in the most important thing is not to be in the most important thing is not to be in the most important thing is not to be in the most important thing is not to be in the most important thing is not to\n",
      "\n",
      "일곱 명의 사망자가 발생했다.\n",
      "=> cnn a man who was in the world s largest economy , but the number of the company said .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84: 100%|██████████| 148/148 [01:13<00:00,  2.00it/s, Loss 1.8619]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84 Result\n",
      "오바마는 대통령이다.\n",
      "=> obama s office said he said .\n",
      "\n",
      "시민들은 도시 속에 산다.\n",
      "=> it s .\n",
      "\n",
      "커피는 필요 없다.\n",
      "=> the report said .\n",
      "\n",
      "일곱 명의 사망자가 발생했다.\n",
      "=> cnn a group of the company said .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85: 100%|██████████| 148/148 [01:13<00:00,  2.01it/s, Loss 1.8366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85 Result\n",
      "오바마는 대통령이다.\n",
      "=> obama s office said he said .\n",
      "\n",
      "시민들은 도시 속에 산다.\n",
      "=> he was not to be in the same time , but the company said .\n",
      "\n",
      "커피는 필요 없다.\n",
      "=> the agency said .\n",
      "\n",
      "일곱 명의 사망자가 발생했다.\n",
      "=> cnn a group of the u . s . , who was in the world s most important thing of the company was not to the world s largest economy , but the number of the company said .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86: 100%|██████████| 148/148 [01:13<00:00,  2.01it/s, Loss 1.8273]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 Result\n",
      "오바마는 대통령이다.\n",
      "=> obama s office was not to be a very uptight man .\n",
      "\n",
      "시민들은 도시 속에 산다.\n",
      "=> he was not to be in the most important thing .\n",
      "\n",
      "커피는 필요 없다.\n",
      "=> the number of the number of people have been in the most important thing to be in the most important thing to be in the most important thing to be in the most important thing to be in the most important thing to be in the most important thing to be in the most important thing to be in\n",
      "\n",
      "일곱 명의 사망자가 발생했다.\n",
      "=> although the u . s . , was arrested by the world s most of the most important thing is not to the world s largest economy , but the number of the company was not to the world s largest economy , but the number of the company was not to the world s largest economy , but\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87: 100%|██████████| 148/148 [01:13<00:00,  2.01it/s, Loss 1.8220]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87 Result\n",
      "오바마는 대통령이다.\n",
      "=> obama s office said he said .\n",
      "\n",
      "시민들은 도시 속에 산다.\n",
      "=> it s .\n",
      "\n",
      "커피는 필요 없다.\n",
      "=> the number of the number of the number of people have been to be in the same time , but the woman , who said .\n",
      "\n",
      "일곱 명의 사망자가 발생했다.\n",
      "=> although the u . s . , who was in the world s largest economy , but the number of the company said .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88: 100%|██████████| 148/148 [01:13<00:00,  2.00it/s, Loss 1.8185]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88 Result\n",
      "오바마는 대통령이다.\n",
      "=> obama s office said he said .\n",
      "\n",
      "시민들은 도시 속에 산다.\n",
      "=> it s .\n",
      "\n",
      "커피는 필요 없다.\n",
      "=> the man was not to be in the same time , but the woman was not to be in the same time , but the woman was not to be in the same time , but the woman was not to be in the same time , but the woman was not to be in the same time ,\n",
      "\n",
      "일곱 명의 사망자가 발생했다.\n",
      "=> i think it was not to the world s largest economy and that the people were in the world s largest economy and that the people were in the world s largest economy and that the people were in the world s largest economy and that the people were in the world s largest economy and that the people\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89: 100%|██████████| 148/148 [01:13<00:00,  2.01it/s, Loss 1.8159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89 Result\n",
      "오바마는 대통령이다.\n",
      "=> obama s office said he said .\n",
      "\n",
      "시민들은 도시 속에 산다.\n",
      "=> he was not to be in the best of the most important thing is not to be in the best of the most important thing is not to be in the best of the most important thing is not to be in the best of the most important thing is not to be in the best of the most\n",
      "\n",
      "커피는 필요 없다.\n",
      "=> the number of the number of the number of the first time , a spokesman for the most important thing is a year .\n",
      "\n",
      "일곱 명의 사망자가 발생했다.\n",
      "=> the united states was in the world s .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90: 100%|██████████| 148/148 [01:13<00:00,  2.00it/s, Loss 1.8127]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90 Result\n",
      "오바마는 대통령이다.\n",
      "=> obama s office said he said .\n",
      "\n",
      "시민들은 도시 속에 산다.\n",
      "=> he was not to be in the same time , but the company said .\n",
      "\n",
      "커피는 필요 없다.\n",
      "=> the agency said .\n",
      "\n",
      "일곱 명의 사망자가 발생했다.\n",
      "=> the united states is the first time .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 512\n",
    "EPOCHS = 30\n",
    "EPOCH_START = 60 # 이어서 학습할 때 사용, 중단한 epoch에서 시작\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    # 데이터셋 섞기\n",
    "    idx_list = list(range(0, kor_sequences.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)    # tqdm\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss = train_step(kor_sequences[idx:idx+BATCH_SIZE], # batch를 직접 지정\n",
    "                                eng_sequences[idx:idx+BATCH_SIZE],\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                eng_tokenizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (EPOCH_START + epoch + 1))    # tqdm\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))    # tqdm\n",
    "    \n",
    "    # 학습한 결과 확인하기\n",
    "    print('Epoch', EPOCH_START + epoch + 1, 'Result')\n",
    "    for kor in [\"오바마는 대통령이다.\", \"시민들은 도시 속에 산다.\", \"커피는 필요 없다.\", \"일곱 명의 사망자가 발생했다.\"]:\n",
    "        print(kor)\n",
    "        print('=>', predict(kor, encoder, decoder))\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ae541e",
   "metadata": {},
   "source": [
    "## Epoch 10 Result\n",
    "|한국어|영어|\n",
    "|-|-|\n",
    "|오바마는 대통령이다.|the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the|\n",
    "|시민들은 도시 속에 산다.|the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the|\n",
    "|커피는 필요 없다.|the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the|\n",
    "|일곱 명의 사망자가 발생했다.|the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the|\n",
    "\n",
    "## Epoch 20 Result\n",
    "|한국어|영어|\n",
    "|-|-|\n",
    "|오바마는 대통령이다.|the government s .|\n",
    "|시민들은 도시 속에 산다.|the u . s . s .|\n",
    "|커피는 필요 없다.|the government s .|\n",
    "|일곱 명의 사망자가 발생했다.|the government s .|\n",
    "\n",
    "## Epoch 30 Result\n",
    "|한국어|영어|\n",
    "|-|-|\n",
    "|오바마는 대통령이다.|obama is a year .|\n",
    "|시민들은 도시 속에 산다.|the government is a year .|\n",
    "|커피는 필요 없다.|the government is a year old , the government is a year old , the government is a year old , the government is a year old , the government is a year old , the government is a year old , the government is a year old , the government is a year old , the government is|\n",
    "|일곱 명의 사망자가 발생했다.|the government is a new york s party , the government is a new york s party , the government is a new york s party , the government is a new york s party , the government is a new york s party , the government is a new york s party , the government is a new|\n",
    "\n",
    "## Epoch 40 Result\n",
    "|한국어|영어|\n",
    "|-|-|\n",
    "|오바마는 대통령이다.|obama is the first time of the government is a few years .|\n",
    "|시민들은 도시 속에 산다.|he said .|\n",
    "|커피는 필요 없다.|the new york cnn a spokesman for the first time is a year old .|\n",
    "|일곱 명의 사망자가 발생했다.|cnn the government is the first time in the united states , 000 people were killed in the world s largest , 000 people were killed in the world s largest , 000 people were killed in the world s largest , 000 people were killed in the world s largest , 000 people were killed in the world|\n",
    "\n",
    "## Epoch 50 Result\n",
    "|한국어|영어|\n",
    "|-|-|\n",
    "|오바마는 대통령이다.|obama s office said .|\n",
    "|시민들은 도시 속에 산다.|he said .|\n",
    "|커피는 필요 없다.|the new york times reports said .|\n",
    "|일곱 명의 사망자가 발생했다.|cnn a man who was a year .|\n",
    "\n",
    "## Epoch 60 Result\n",
    "|한국어|영어|\n",
    "|-|-|\n",
    "|오바마는 대통령이다.|obama s office said .|\n",
    "|시민들은 도시 속에 산다.|he said .|\n",
    "|커피는 필요 없다.|the new york times reports said .|\n",
    "|일곱 명의 사망자가 발생했다.|cnn a new york times reports said .|\n",
    "\n",
    "## Epoch 70 Result\n",
    "|한국어|영어|\n",
    "|-|-|\n",
    "|오바마는 대통령이다.|obama s office said he said .|\n",
    "|시민들은 도시 속에 산다.|he said .|\n",
    "|커피는 필요 없다.|the number of the number of the people were not to be in the first time , the police chief executive of the people were not to be in the first time , the police chief executive of the people were not to be in the first time , the police chief executive of the people were not to|\n",
    "|일곱 명의 사망자가 발생했다.|although the u . s . , who was not to the world s largest economy , he said .|\n",
    "\n",
    "## Epoch 80 Result\n",
    "|한국어|영어|\n",
    "|-|-|\n",
    "|오바마는 대통령이다.|obama s office said he said .|\n",
    "|시민들은 도시 속에 산다.|he said .|\n",
    "|커피는 필요 없다.|the woman was not to be in the most important thing is a lot of dollars .|\n",
    "|일곱 명의 사망자가 발생했다.|although the u . s . , who was not to the world s largest economy , he said .|\n",
    "\n",
    "## Epoch 90 Result\n",
    "|한국어|영어|\n",
    "|-|-|\n",
    "|오바마는 대통령이다.|obama s office said he said .|\n",
    "|시민들은 도시 속에 산다.|he was not to be in the same time , but the company said .|\n",
    "|커피는 필요 없다.|the agency said .|\n",
    "|일곱 명의 사망자가 발생했다.|the united states is the first time .|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5ea64ea2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dec_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_305/1145870317.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"오바마는 대통령이다.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_305/1145870317.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(sentence, encoder, decoder)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_305/1145870317.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(sentence, encoder, decoder)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# 어텐션 가중치 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# 전처리\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dec_train' is not defined"
     ]
    }
   ],
   "source": [
    "def evaluate(sentence, encoder, decoder):\n",
    "    # 어텐션 가중치 저장\n",
    "    attention = np.zeros((dec_train.shape[-1], enc_train.shape[-1]))\n",
    "    \n",
    "    # 전처리\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    sentence = mecab.morphs(sentence)\n",
    "    inputs = kor_tokenizer.texts_to_sequences([sentence])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                           maxlen=85,\n",
    "                                                           padding='post')\n",
    "    # 번역 결과를 담을 문자열 변수\n",
    "    result = ''\n",
    "\n",
    "    # 인코더\n",
    "    inputs = tf.reverse(inputs, [-1])\n",
    "    enc_out = encoder(inputs)\n",
    "\n",
    "    # 디코더\n",
    "    dec_hidden = enc_out[:, -1]\n",
    "    dec_input = tf.expand_dims([dec_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(dec_train.shape[-1]):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        # 어텐션 가중치를 저장\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention[t] = attention_weights.numpy()\n",
    "\n",
    "        # 디코더가 예측한 단어 디코딩\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0]).numpy()\n",
    "\n",
    "        result += dec_tokenizer.index_word[predicted_id] + ' '\n",
    "        \n",
    "        # 종료 토큰이 나오면 즉시 종료\n",
    "        if dec_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention\n",
    "\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def translate(sentence, encoder, decoder):\n",
    "    result, sentence, attention = evaluate(sentence, encoder, decoder)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention = attention[:len(result.split()), :len(sentence.split())]\n",
    "    plot_attention(attention, sentence.split(), result.split(' '))\n",
    "\n",
    "\n",
    "translate(\"오바마는 대통령이다.\", encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1692e1e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
